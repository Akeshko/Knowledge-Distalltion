{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vTSy-uiXWbu",
        "outputId": "c2bb19f8-33be-43bf-9a1a-1a8e39c7d754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "Loading datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 55.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.90MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 15.0MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully!\n",
            "Loading teacher model...\n",
            "Teacher model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Common Data Processing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(3),  # Convert to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "print(\"Datasets loaded successfully!\")\n",
        "\n",
        "# Load teacher model\n",
        "print(\"Loading teacher model...\")\n",
        "teacher_model = models.resnet50(weights=None)\n",
        "teacher_model.fc = nn.Linear(2048, 10)\n",
        "teacher_model.load_state_dict(torch.load(\"/content/drive/MyDrive/resnet50_mnist.pth\", map_location=device))\n",
        "teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "print(\"Teacher model loaded successfully!\")\n",
        "\n",
        "# Function to calculate test accuracy\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    accuracy = 100. * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize student model FROM SCRATCH (no pretrained weights)\n",
        "student_soft = models.resnet18(weights=None)  # Explicitly no pretrained weights\n",
        "student_soft.fc = nn.Linear(512, 10)  # Adjust final layer for 10-class MNIST\n",
        "student_soft.to(device)\n",
        "print(\"Student model initialized from scratch (no pretrained weights)\")\n",
        "\n",
        "# Distillation loss (unchanged)\n",
        "class SoftTargetLoss(nn.Module):\n",
        "    def __init__(self, T=3.0, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.alpha = alpha\n",
        "        self.kl_div = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        soft_loss = self.kl_div(\n",
        "            F.log_softmax(student_logits/self.T, dim=1),\n",
        "            F.softmax(teacher_logits/self.T, dim=1)\n",
        "        ) * (self.T ** 2)\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "\n",
        "# Rest of your code remains the same...\n",
        "optimizer = optim.Adam(student_soft.parameters(), lr=0.001)\n",
        "criterion = SoftTargetLoss(T=3.0, alpha=0.5)\n",
        "\n",
        "# Verify teacher model\n",
        "print(\"Teacher model verification:\")\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        teacher_output = teacher_model(dummy_input)\n",
        "    print(\"✓ Teacher model works (output shape:\", teacher_output.shape, \")\")\n",
        "except Exception as e:\n",
        "    print(\"✗ Teacher model failed:\", e)\n",
        "    raise\n",
        "\n",
        "# Training loop (unchanged)\n",
        "print(\"\\nStarting Soft Target Distillation Training...\")\n",
        "try:\n",
        "    for epoch in range(10):\n",
        "        student_soft.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Test data loading\n",
        "        print(f\"\\nEpoch {epoch+1} - Testing data loader...\")\n",
        "        test_images, test_labels = next(iter(train_loader))\n",
        "        print(\"✓ Data loader works (batch shape:\", test_images.shape, \")\")\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(images)\n",
        "\n",
        "            student_outputs = student_soft(images)\n",
        "            loss = criterion(student_outputs, teacher_outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_acc = evaluate_model(student_soft, train_loader)\n",
        "        test_acc = evaluate_model(student_soft, test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/10], Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Training failed:\", e)\n",
        "    raise\n",
        "\n",
        "# Save final model\n",
        "torch.save(student_soft.state_dict(), \"/content/drive/MyDrive/soft_target_final.pth\")\n",
        "print(\"✓ Model saved successfully\")\n",
        "print(\"Soft Target Distillation Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvtsoTJzXreV",
        "outputId": "5f4a26ea-d49f-49d3-a547-3fafb089686a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student model initialized from scratch (no pretrained weights)\n",
            "Teacher model verification:\n",
            "✓ Teacher model works (output shape: torch.Size([1, 10]) )\n",
            "\n",
            "Starting Soft Target Distillation Training...\n",
            "\n",
            "Epoch 1 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 2.0042\n",
            "Batch 50: Loss = 0.8993\n",
            "Batch 100: Loss = 0.7630\n",
            "Batch 150: Loss = 0.7103\n",
            "Batch 200: Loss = 0.6254\n",
            "Batch 250: Loss = 0.6481\n",
            "Batch 300: Loss = 0.6947\n",
            "Batch 350: Loss = 0.6412\n",
            "Batch 400: Loss = 0.7024\n",
            "Batch 450: Loss = 0.7153\n",
            "Batch 500: Loss = 0.7269\n",
            "Batch 550: Loss = 0.7038\n",
            "Batch 600: Loss = 0.6642\n",
            "Batch 650: Loss = 0.6393\n",
            "Batch 700: Loss = 0.7129\n",
            "Batch 750: Loss = 0.6483\n",
            "Batch 800: Loss = 0.6238\n",
            "Batch 850: Loss = 0.6580\n",
            "Batch 900: Loss = 0.6606\n",
            "Epoch [1/10], Loss: 0.6921, Train Acc: 93.11%, Test Acc: 93.01%\n",
            "\n",
            "Epoch 2 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.6160\n",
            "Batch 50: Loss = 0.6264\n",
            "Batch 100: Loss = 0.6364\n",
            "Batch 150: Loss = 0.7304\n",
            "Batch 200: Loss = 0.7362\n",
            "Batch 250: Loss = 0.6602\n",
            "Batch 300: Loss = 0.6325\n",
            "Batch 350: Loss = 0.6330\n",
            "Batch 400: Loss = 0.6558\n",
            "Batch 450: Loss = 0.5910\n",
            "Batch 500: Loss = 0.7198\n",
            "Batch 550: Loss = 0.7003\n",
            "Batch 600: Loss = 0.6188\n",
            "Batch 650: Loss = 0.5943\n",
            "Batch 700: Loss = 0.6596\n",
            "Batch 750: Loss = 0.6895\n",
            "Batch 800: Loss = 0.7606\n",
            "Batch 850: Loss = 0.6101\n",
            "Batch 900: Loss = 0.7096\n",
            "Epoch [2/10], Loss: 0.6446, Train Acc: 93.36%, Test Acc: 93.22%\n",
            "\n",
            "Epoch 3 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.6476\n",
            "Batch 50: Loss = 0.6199\n",
            "Batch 100: Loss = 0.6179\n",
            "Batch 150: Loss = 0.7243\n",
            "Batch 200: Loss = 0.6525\n",
            "Batch 250: Loss = 0.6215\n",
            "Batch 300: Loss = 0.6398\n",
            "Batch 350: Loss = 0.6102\n",
            "Batch 400: Loss = 0.6551\n",
            "Batch 450: Loss = 0.5935\n",
            "Batch 500: Loss = 0.6427\n",
            "Batch 550: Loss = 0.5935\n",
            "Batch 600: Loss = 0.6519\n",
            "Batch 650: Loss = 0.6965\n",
            "Batch 700: Loss = 0.6867\n",
            "Batch 750: Loss = 0.6579\n",
            "Batch 800: Loss = 0.6996\n",
            "Batch 850: Loss = 0.7265\n",
            "Batch 900: Loss = 0.5666\n",
            "Epoch [3/10], Loss: 0.6354, Train Acc: 94.73%, Test Acc: 94.53%\n",
            "\n",
            "Epoch 4 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.5858\n",
            "Batch 50: Loss = 0.5947\n",
            "Batch 100: Loss = 0.6054\n",
            "Batch 150: Loss = 0.5963\n",
            "Batch 200: Loss = 0.6411\n",
            "Batch 250: Loss = 0.6999\n",
            "Batch 300: Loss = 0.6235\n",
            "Batch 350: Loss = 0.6144\n",
            "Batch 400: Loss = 0.6463\n",
            "Batch 450: Loss = 0.6162\n",
            "Batch 500: Loss = 0.6777\n",
            "Batch 550: Loss = 0.6376\n",
            "Batch 600: Loss = 0.6724\n",
            "Batch 650: Loss = 0.6066\n",
            "Batch 700: Loss = 0.5949\n",
            "Batch 750: Loss = 0.6716\n",
            "Batch 800: Loss = 0.6739\n",
            "Batch 850: Loss = 0.5813\n",
            "Batch 900: Loss = 0.5990\n",
            "Epoch [4/10], Loss: 0.6307, Train Acc: 97.78%, Test Acc: 97.67%\n",
            "\n",
            "Epoch 5 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.5660\n",
            "Batch 50: Loss = 0.6249\n",
            "Batch 100: Loss = 0.6355\n",
            "Batch 150: Loss = 0.6836\n",
            "Batch 200: Loss = 0.6803\n",
            "Batch 250: Loss = 0.6213\n",
            "Batch 300: Loss = 0.6564\n",
            "Batch 350: Loss = 0.6261\n",
            "Batch 400: Loss = 0.6172\n",
            "Batch 450: Loss = 0.5700\n",
            "Batch 500: Loss = 0.6266\n",
            "Batch 550: Loss = 0.6289\n",
            "Batch 600: Loss = 0.6002\n",
            "Batch 650: Loss = 0.5960\n",
            "Batch 700: Loss = 0.6141\n",
            "Batch 750: Loss = 0.6981\n",
            "Batch 800: Loss = 0.6700\n",
            "Batch 850: Loss = 0.6097\n",
            "Batch 900: Loss = 0.6277\n",
            "Epoch [5/10], Loss: 0.6281, Train Acc: 92.56%, Test Acc: 91.83%\n",
            "\n",
            "Epoch 6 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.6314\n",
            "Batch 50: Loss = 0.6056\n",
            "Batch 100: Loss = 0.5374\n",
            "Batch 150: Loss = 0.7269\n",
            "Batch 200: Loss = 0.5850\n",
            "Batch 250: Loss = 0.6454\n",
            "Batch 300: Loss = 0.6130\n",
            "Batch 350: Loss = 0.6045\n",
            "Batch 400: Loss = 0.6173\n",
            "Batch 450: Loss = 0.6490\n",
            "Batch 500: Loss = 0.5664\n",
            "Batch 550: Loss = 0.6640\n",
            "Batch 600: Loss = 0.5674\n",
            "Batch 650: Loss = 0.7224\n",
            "Batch 700: Loss = 0.6378\n",
            "Batch 750: Loss = 0.6276\n",
            "Batch 800: Loss = 0.5791\n",
            "Batch 850: Loss = 0.5853\n",
            "Batch 900: Loss = 0.5696\n",
            "Epoch [6/10], Loss: 0.6238, Train Acc: 94.82%, Test Acc: 94.39%\n",
            "\n",
            "Epoch 7 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.6131\n",
            "Batch 50: Loss = 0.5960\n",
            "Batch 100: Loss = 0.5915\n",
            "Batch 150: Loss = 0.6608\n",
            "Batch 200: Loss = 0.6168\n",
            "Batch 250: Loss = 0.5967\n",
            "Batch 300: Loss = 0.5637\n",
            "Batch 350: Loss = 0.7534\n",
            "Batch 400: Loss = 0.6355\n",
            "Batch 450: Loss = 0.5958\n",
            "Batch 500: Loss = 0.6315\n",
            "Batch 550: Loss = 0.6414\n",
            "Batch 600: Loss = 0.6684\n",
            "Batch 650: Loss = 0.6926\n",
            "Batch 700: Loss = 0.6548\n",
            "Batch 750: Loss = 0.6153\n",
            "Batch 800: Loss = 0.6051\n",
            "Batch 850: Loss = 0.5284\n",
            "Batch 900: Loss = 0.6050\n",
            "Epoch [7/10], Loss: 0.6228, Train Acc: 98.81%, Test Acc: 98.55%\n",
            "\n",
            "Epoch 8 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.5918\n",
            "Batch 50: Loss = 0.6708\n",
            "Batch 100: Loss = 0.6066\n",
            "Batch 150: Loss = 0.6494\n",
            "Batch 200: Loss = 0.6240\n",
            "Batch 250: Loss = 0.5616\n",
            "Batch 300: Loss = 0.5961\n",
            "Batch 350: Loss = 0.5918\n",
            "Batch 400: Loss = 0.6185\n",
            "Batch 450: Loss = 0.6794\n",
            "Batch 500: Loss = 0.6170\n",
            "Batch 550: Loss = 0.5900\n",
            "Batch 600: Loss = 0.6001\n",
            "Batch 650: Loss = 0.6161\n",
            "Batch 700: Loss = 0.6302\n",
            "Batch 750: Loss = 0.6124\n",
            "Batch 800: Loss = 0.5554\n",
            "Batch 850: Loss = 0.6158\n",
            "Batch 900: Loss = 0.5944\n",
            "Epoch [8/10], Loss: 0.6209, Train Acc: 98.22%, Test Acc: 97.88%\n",
            "\n",
            "Epoch 9 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.5988\n",
            "Batch 50: Loss = 0.6510\n",
            "Batch 100: Loss = 0.5947\n",
            "Batch 150: Loss = 0.6536\n",
            "Batch 200: Loss = 0.6090\n",
            "Batch 250: Loss = 0.6524\n",
            "Batch 300: Loss = 0.6317\n",
            "Batch 350: Loss = 0.5850\n",
            "Batch 400: Loss = 0.6174\n",
            "Batch 450: Loss = 0.5449\n",
            "Batch 500: Loss = 0.6053\n",
            "Batch 550: Loss = 0.6414\n",
            "Batch 600: Loss = 0.6486\n",
            "Batch 650: Loss = 0.6208\n",
            "Batch 700: Loss = 0.5834\n",
            "Batch 750: Loss = 0.6057\n",
            "Batch 800: Loss = 0.5870\n",
            "Batch 850: Loss = 0.6967\n",
            "Batch 900: Loss = 0.5801\n",
            "Epoch [9/10], Loss: 0.6180, Train Acc: 95.50%, Test Acc: 94.81%\n",
            "\n",
            "Epoch 10 - Testing data loader...\n",
            "✓ Data loader works (batch shape: torch.Size([64, 3, 28, 28]) )\n",
            "Batch 0: Loss = 0.6188\n",
            "Batch 50: Loss = 0.5838\n",
            "Batch 100: Loss = 0.5803\n",
            "Batch 150: Loss = 0.6307\n",
            "Batch 200: Loss = 0.6132\n",
            "Batch 250: Loss = 0.7058\n",
            "Batch 300: Loss = 0.6116\n",
            "Batch 350: Loss = 0.6060\n",
            "Batch 400: Loss = 0.6891\n",
            "Batch 450: Loss = 0.6765\n",
            "Batch 500: Loss = 0.6920\n",
            "Batch 550: Loss = 0.6573\n",
            "Batch 600: Loss = 0.6472\n",
            "Batch 650: Loss = 0.6003\n",
            "Batch 700: Loss = 0.5953\n",
            "Batch 750: Loss = 0.6320\n",
            "Batch 800: Loss = 0.7456\n",
            "Batch 850: Loss = 0.6382\n",
            "Batch 900: Loss = 0.6235\n",
            "Epoch [10/10], Loss: 0.6177, Train Acc: 95.50%, Test Acc: 94.96%\n",
            "✓ Model saved successfully\n",
            "Soft Target Distillation Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning final evaluation on test set...\")\n",
        "student_soft.eval()  # Set to evaluation mode\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = student_soft(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "final_test_acc = 100.0 * total_correct / total_samples\n",
        "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaPoxfhSpxbu",
        "outputId": "315bb682-3014-4a51-bc8b-936dcc271d3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running final evaluation on test set...\n",
            "Final Test Accuracy: 94.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize models\n",
        "teacher_model = models.resnet50(weights=None)\n",
        "teacher_model.fc = nn.Linear(2048, 10)\n",
        "teacher_model.load_state_dict(torch.load(\"/content/drive/MyDrive/resnet50_mnist.pth\"))\n",
        "teacher_model.to(device).eval()\n",
        "\n",
        "# Student model with attention hooks\n",
        "class ResNet18AT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet18(weights=None)\n",
        "        self.model.fc = nn.Linear(512, 10)\n",
        "\n",
        "        # Register hooks for attention maps\n",
        "        self.attention_maps = []\n",
        "        self.handles = []\n",
        "\n",
        "        # Layers we'll use for attention transfer (typically after residual blocks)\n",
        "        target_layers = ['layer1', 'layer2', 'layer3']\n",
        "        for name, layer in self.model.named_modules():\n",
        "            if name in target_layers:\n",
        "                self.handles.append(layer.register_forward_hook(self._hook_save_activation))\n",
        "\n",
        "    def _hook_save_activation(self, module, input, output):\n",
        "        self.attention_maps.append(output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.attention_maps = []  # Clear previous maps\n",
        "        return self.model(x)\n",
        "\n",
        "    def close(self):\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n",
        "\n",
        "student_at = ResNet18AT().to(device)\n",
        "\n",
        "# Attention Transfer Loss\n",
        "class AttentionTransferLoss(nn.Module):\n",
        "    def __init__(self, beta=1000):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits,\n",
        "               student_attention, teacher_attention, labels):\n",
        "        # Standard cross entropy\n",
        "        ce_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Attention transfer loss\n",
        "        at_loss = 0\n",
        "        for s_att, t_att in zip(student_attention, teacher_attention):\n",
        "            s_att = self._attention_map(s_att)\n",
        "            t_att = self._attention_map(t_att)\n",
        "            at_loss += F.mse_loss(s_att, t_att)\n",
        "\n",
        "        return ce_loss + self.beta * at_loss\n",
        "\n",
        "    def _attention_map(self, x):\n",
        "        return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))\n",
        "\n",
        "# Training setup\n",
        "optimizer = optim.Adam(student_at.parameters(), lr=0.001)\n",
        "criterion = AttentionTransferLoss(beta=1000)\n",
        "\n",
        "# Data loading (same as before)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Teacher attention extraction function\n",
        "def get_teacher_attention(teacher, x):\n",
        "    attention_maps = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        attention_maps.append(output)\n",
        "\n",
        "    handles = []\n",
        "    target_layers = ['layer1', 'layer2', 'layer3']\n",
        "    for name, layer in teacher.named_modules():\n",
        "        if name in target_layers:\n",
        "            handles.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = teacher(x)\n",
        "\n",
        "    for handle in handles:\n",
        "        handle.remove()\n",
        "\n",
        "    return attention_maps\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting Attention Transfer Distillation...\")\n",
        "for epoch in range(10):\n",
        "    student_at.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get teacher attention and outputs\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(images)\n",
        "            teacher_attention = get_teacher_attention(teacher_model, images)\n",
        "\n",
        "        # Forward pass through student\n",
        "        student_outputs = student_at(images)\n",
        "        student_attention = student_at.attention_maps\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(student_outputs, teacher_outputs,\n",
        "                        student_attention, teacher_attention, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    student_at.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = student_at(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "          f\"Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "# Final evaluation\n",
        "student_at.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = student_at(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "final_acc = 100 * correct / total\n",
        "print(f\"\\nFinal Test Accuracy: {final_acc:.2f}%\")\n",
        "\n",
        "# Save model\n",
        "torch.save(student_at.model.state_dict(), \"/content/drive/MyDrive/attention_transfer_final.pth\")\n",
        "print(\"✓ Model saved successfully\")\n",
        "student_at.close()  # Remove hooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYSaIAD9XsTU",
        "outputId": "172f7fd4-c9da-42a8-e696-c44e377f1380"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Attention Transfer Distillation...\n",
            "Batch 0: Loss = 81.3155\n",
            "Batch 50: Loss = 2.9661\n",
            "Batch 100: Loss = 2.4291\n",
            "Batch 150: Loss = 1.5411\n",
            "Batch 200: Loss = 1.8795\n",
            "Batch 250: Loss = 1.3226\n",
            "Batch 300: Loss = 1.0884\n",
            "Batch 350: Loss = 1.4972\n",
            "Batch 400: Loss = 2.6888\n",
            "Batch 450: Loss = 1.2004\n",
            "Batch 500: Loss = 0.8917\n",
            "Batch 550: Loss = 0.8630\n",
            "Batch 600: Loss = 0.8361\n",
            "Batch 650: Loss = 0.7674\n",
            "Batch 700: Loss = 0.9480\n",
            "Batch 750: Loss = 0.8772\n",
            "Batch 800: Loss = 0.7945\n",
            "Batch 850: Loss = 0.6903\n",
            "Batch 900: Loss = 0.7994\n",
            "Epoch [1/10], Loss: 1.6736, Test Acc: 93.72%\n",
            "Batch 0: Loss = 0.6153\n",
            "Batch 50: Loss = 0.6735\n",
            "Batch 100: Loss = 0.7624\n",
            "Batch 150: Loss = 0.9390\n",
            "Batch 200: Loss = 0.6888\n",
            "Batch 250: Loss = 0.6382\n",
            "Batch 300: Loss = 0.5123\n",
            "Batch 350: Loss = 0.3936\n",
            "Batch 400: Loss = 0.6259\n",
            "Batch 450: Loss = 0.6275\n",
            "Batch 500: Loss = 0.7583\n",
            "Batch 550: Loss = 0.6020\n",
            "Batch 600: Loss = 1.0465\n",
            "Batch 650: Loss = 0.4374\n",
            "Batch 700: Loss = 0.5133\n",
            "Batch 750: Loss = 0.6482\n",
            "Batch 800: Loss = 0.3375\n",
            "Batch 850: Loss = 0.4446\n",
            "Batch 900: Loss = 0.2891\n",
            "Epoch [2/10], Loss: 0.5701, Test Acc: 95.14%\n",
            "Batch 0: Loss = 0.3399\n",
            "Batch 50: Loss = 0.3564\n",
            "Batch 100: Loss = 0.3969\n",
            "Batch 150: Loss = 0.3408\n",
            "Batch 200: Loss = 0.4695\n",
            "Batch 250: Loss = 0.6164\n",
            "Batch 300: Loss = 0.4369\n",
            "Batch 350: Loss = 0.3622\n",
            "Batch 400: Loss = 0.4974\n",
            "Batch 450: Loss = 0.3714\n",
            "Batch 500: Loss = 0.3036\n",
            "Batch 550: Loss = 0.5209\n",
            "Batch 600: Loss = 0.4416\n",
            "Batch 650: Loss = 0.2918\n",
            "Batch 700: Loss = 0.4287\n",
            "Batch 750: Loss = 0.3917\n",
            "Batch 800: Loss = 0.3262\n",
            "Batch 850: Loss = 0.4221\n",
            "Batch 900: Loss = 0.3325\n",
            "Epoch [3/10], Loss: 0.4336, Test Acc: 95.49%\n",
            "Batch 0: Loss = 0.5761\n",
            "Batch 50: Loss = 0.3077\n",
            "Batch 100: Loss = 0.2406\n",
            "Batch 150: Loss = 0.2426\n",
            "Batch 200: Loss = 0.4710\n",
            "Batch 250: Loss = 0.4124\n",
            "Batch 300: Loss = 0.2757\n",
            "Batch 350: Loss = 0.5648\n",
            "Batch 400: Loss = 0.4472\n",
            "Batch 450: Loss = 0.2939\n",
            "Batch 500: Loss = 0.2975\n",
            "Batch 550: Loss = 0.3419\n",
            "Batch 600: Loss = 0.2715\n",
            "Batch 650: Loss = 0.2855\n",
            "Batch 700: Loss = 0.3383\n",
            "Batch 750: Loss = 0.2965\n",
            "Batch 800: Loss = 0.2286\n",
            "Batch 850: Loss = 0.3337\n",
            "Batch 900: Loss = 0.3529\n",
            "Epoch [4/10], Loss: 0.3753, Test Acc: 97.73%\n",
            "Batch 0: Loss = 0.3219\n",
            "Batch 50: Loss = 0.2370\n",
            "Batch 100: Loss = 0.2718\n",
            "Batch 150: Loss = 0.2803\n",
            "Batch 200: Loss = 0.3131\n",
            "Batch 250: Loss = 0.2464\n",
            "Batch 300: Loss = 0.2683\n",
            "Batch 350: Loss = 0.4160\n",
            "Batch 400: Loss = 0.3958\n",
            "Batch 450: Loss = 0.3729\n",
            "Batch 500: Loss = 0.2630\n",
            "Batch 550: Loss = 0.5945\n",
            "Batch 600: Loss = 0.2847\n",
            "Batch 650: Loss = 0.2481\n",
            "Batch 700: Loss = 0.2454\n",
            "Batch 750: Loss = 0.3387\n",
            "Batch 800: Loss = 0.3619\n",
            "Batch 850: Loss = 0.2055\n",
            "Batch 900: Loss = 0.2876\n",
            "Epoch [5/10], Loss: 0.3393, Test Acc: 98.11%\n",
            "Batch 0: Loss = 0.3109\n",
            "Batch 50: Loss = 0.3257\n",
            "Batch 100: Loss = 0.3150\n",
            "Batch 150: Loss = 0.3130\n",
            "Batch 200: Loss = 0.2159\n",
            "Batch 250: Loss = 0.1816\n",
            "Batch 300: Loss = 0.3276\n",
            "Batch 350: Loss = 0.3087\n",
            "Batch 400: Loss = 0.2994\n",
            "Batch 450: Loss = 0.2040\n",
            "Batch 500: Loss = 0.4328\n",
            "Batch 550: Loss = 0.4043\n",
            "Batch 600: Loss = 0.2369\n",
            "Batch 650: Loss = 0.2699\n",
            "Batch 700: Loss = 0.3465\n",
            "Batch 750: Loss = 0.2344\n",
            "Batch 800: Loss = 0.2408\n",
            "Batch 850: Loss = 0.2340\n",
            "Batch 900: Loss = 0.3546\n",
            "Epoch [6/10], Loss: 0.3045, Test Acc: 98.03%\n",
            "Batch 0: Loss = 0.1894\n",
            "Batch 50: Loss = 0.2958\n",
            "Batch 100: Loss = 0.2331\n",
            "Batch 150: Loss = 0.2953\n",
            "Batch 200: Loss = 0.2465\n",
            "Batch 250: Loss = 0.3241\n",
            "Batch 300: Loss = 0.2257\n",
            "Batch 350: Loss = 0.2397\n",
            "Batch 400: Loss = 0.2608\n",
            "Batch 450: Loss = 0.2389\n",
            "Batch 500: Loss = 0.2997\n",
            "Batch 550: Loss = 0.3520\n",
            "Batch 600: Loss = 0.2199\n",
            "Batch 650: Loss = 0.3679\n",
            "Batch 700: Loss = 0.1758\n",
            "Batch 750: Loss = 0.3326\n",
            "Batch 800: Loss = 0.1962\n",
            "Batch 850: Loss = 0.2102\n",
            "Batch 900: Loss = 0.2803\n",
            "Epoch [7/10], Loss: 0.2645, Test Acc: 98.24%\n",
            "Batch 0: Loss = 0.3366\n",
            "Batch 50: Loss = 0.3808\n",
            "Batch 100: Loss = 0.2968\n",
            "Batch 150: Loss = 0.2299\n",
            "Batch 200: Loss = 0.2263\n",
            "Batch 250: Loss = 0.1610\n",
            "Batch 300: Loss = 0.2873\n",
            "Batch 350: Loss = 0.2923\n",
            "Batch 400: Loss = 0.2490\n",
            "Batch 450: Loss = 0.2850\n",
            "Batch 500: Loss = 0.1484\n",
            "Batch 550: Loss = 0.1994\n",
            "Batch 600: Loss = 0.1228\n",
            "Batch 650: Loss = 0.1851\n",
            "Batch 700: Loss = 0.2723\n",
            "Batch 750: Loss = 0.3655\n",
            "Batch 800: Loss = 0.2974\n",
            "Batch 850: Loss = 0.2097\n",
            "Batch 900: Loss = 0.2603\n",
            "Epoch [8/10], Loss: 0.2454, Test Acc: 97.88%\n",
            "Batch 0: Loss = 0.2291\n",
            "Batch 50: Loss = 0.1594\n",
            "Batch 100: Loss = 0.2626\n",
            "Batch 150: Loss = 0.2626\n",
            "Batch 200: Loss = 0.1892\n",
            "Batch 250: Loss = 0.3738\n",
            "Batch 300: Loss = 0.1697\n",
            "Batch 350: Loss = 0.2707\n",
            "Batch 400: Loss = 0.2150\n",
            "Batch 450: Loss = 0.2412\n",
            "Batch 500: Loss = 0.1718\n",
            "Batch 550: Loss = 0.1919\n",
            "Batch 600: Loss = 0.2008\n",
            "Batch 650: Loss = 0.2910\n",
            "Batch 700: Loss = 0.3186\n",
            "Batch 750: Loss = 0.1473\n",
            "Batch 800: Loss = 0.2137\n",
            "Batch 850: Loss = 0.2104\n",
            "Batch 900: Loss = 0.2492\n",
            "Epoch [9/10], Loss: 0.2308, Test Acc: 98.93%\n",
            "Batch 0: Loss = 0.4230\n",
            "Batch 50: Loss = 0.1675\n",
            "Batch 100: Loss = 0.2149\n",
            "Batch 150: Loss = 0.1447\n",
            "Batch 200: Loss = 0.1955\n",
            "Batch 250: Loss = 0.2351\n",
            "Batch 300: Loss = 0.1571\n",
            "Batch 350: Loss = 0.2687\n",
            "Batch 400: Loss = 0.1616\n",
            "Batch 450: Loss = 0.1883\n",
            "Batch 500: Loss = 0.1542\n",
            "Batch 550: Loss = 0.1922\n",
            "Batch 600: Loss = 0.2296\n",
            "Batch 650: Loss = 0.2818\n",
            "Batch 700: Loss = 0.2454\n",
            "Batch 750: Loss = 0.1917\n",
            "Batch 800: Loss = 0.1406\n",
            "Batch 850: Loss = 0.2194\n",
            "Batch 900: Loss = 0.1979\n",
            "Epoch [10/10], Loss: 0.2140, Test Acc: 98.63%\n",
            "\n",
            "Final Test Accuracy: 98.63%\n",
            "✓ Model saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Simplified ResNet18 definition matching your saved models\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use the standard resnet18 structure without wrapper\n",
        "        self.resnet = models.resnet18(weights=None)\n",
        "        self.resnet.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Initialize two ResNet-18 students\n",
        "student1 = ResNet18().to(device)\n",
        "student2 = ResNet18().to(device)\n",
        "\n",
        "# Load paths (update these with your actual paths)\n",
        "student1_path = \"/content/drive/MyDrive/soft_target_final.pth\"\n",
        "student2_path = \"/content/drive/MyDrive/attention_transfer_final.pth\"\n",
        "\n",
        "# Modified loading function to handle potential architecture mismatches\n",
        "def load_model(model, path):\n",
        "    try:\n",
        "        # Load full state dict\n",
        "        state_dict = torch.load(path, map_location=device)\n",
        "\n",
        "        # Handle case where model was saved as DataParallel\n",
        "        if all(k.startswith('module.') for k in state_dict.keys()):\n",
        "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "        # Handle case where model was saved with 'resnet.' prefix\n",
        "        if all(k.startswith('resnet.') for k in state_dict.keys()):\n",
        "            state_dict = {k.replace('resnet.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "        model.load_state_dict(state_dict, strict=False)  # strict=False to ignore non-matching keys\n",
        "        print(f\"✓ Model loaded successfully from {path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading model from {path}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load models\n",
        "if not load_model(student1, student1_path):\n",
        "    raise RuntimeError(\"Failed to load student1\")\n",
        "if not load_model(student2, student2_path):\n",
        "    raise RuntimeError(\"Failed to load student2\")\n",
        "\n",
        "# DML Loss (bidirectional KL divergence)\n",
        "class DMLLoss(nn.Module):\n",
        "    def __init__(self, T=3.0, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.alpha = alpha\n",
        "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, logits1, logits2, labels):\n",
        "        # Classification loss\n",
        "        ce_loss = (self.ce_loss(logits1, labels) + self.ce_loss(logits2, labels)) / 2\n",
        "\n",
        "        # Mutual learning loss\n",
        "        soft_loss = (self.kl_div(\n",
        "            F.log_softmax(logits1/self.T, dim=1),\n",
        "            F.softmax(logits2/self.T, dim=1)\n",
        "        ) + self.kl_div(\n",
        "            F.log_softmax(logits2/self.T, dim=1),\n",
        "            F.softmax(logits1/self.T, dim=1)\n",
        "        )) * (self.T ** 2) / 2\n",
        "\n",
        "        return self.alpha * ce_loss + (1 - self.alpha) * soft_loss\n",
        "\n",
        "# Training setup\n",
        "optimizer1 = optim.Adam(student1.parameters(), lr=0.0005)\n",
        "optimizer2 = optim.Adam(student2.parameters(), lr=0.0005)\n",
        "criterion = DMLLoss(T=3.0, alpha=0.5)\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# DML Training loop\n",
        "print(\"Starting Deep Mutual Learning between ResNet-18 students...\")\n",
        "for epoch in range(5):\n",
        "    student1.train()\n",
        "    student2.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward passes\n",
        "        outputs1 = student1(images)\n",
        "        outputs2 = student2(images)\n",
        "\n",
        "        # Compute DML loss\n",
        "        loss = criterion(outputs1, outputs2, labels)\n",
        "\n",
        "        # Backward passes\n",
        "        optimizer1.zero_grad()\n",
        "        optimizer2.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    student1_acc = evaluate_model(student1, test_loader)\n",
        "    student2_acc = evaluate_model(student2, test_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/5], Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "          f\"Student1 Acc: {student1_acc:.2f}%, Student2 Acc: {student2_acc:.2f}%\")\n",
        "\n",
        "# Final evaluation\n",
        "final_acc1 = evaluate_model(student1, test_loader)\n",
        "final_acc2 = evaluate_model(student2, test_loader)\n",
        "print(f\"\\nFinal Test Accuracy - Student1: {final_acc1:.2f}%, Student2: {final_acc2:.2f}%\")\n",
        "\n",
        "# Save models\n",
        "torch.save(student1.state_dict(), \"/content/drive/MyDrive/dml_resnet18_student1_final.pth\")\n",
        "torch.save(student2.state_dict(), \"/content/drive/MyDrive/dml_resnet18_student2_final.pth\")\n",
        "print(\"✓ Both models saved successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ8XkisXt9gc",
        "outputId": "7f756068-928d-43ca-c918-c5eaf76fc134"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded successfully from /content/drive/MyDrive/soft_target_final.pth\n",
            "✓ Model loaded successfully from /content/drive/MyDrive/attention_transfer_final.pth\n",
            "Starting Deep Mutual Learning between ResNet-18 students...\n",
            "Batch 0: Loss = 1.6044\n",
            "Batch 50: Loss = 0.2014\n",
            "Batch 100: Loss = 0.1167\n",
            "Batch 150: Loss = 0.0615\n",
            "Batch 200: Loss = 0.1172\n",
            "Batch 250: Loss = 0.0976\n",
            "Batch 300: Loss = 0.2322\n",
            "Batch 350: Loss = 0.0875\n",
            "Batch 400: Loss = 0.1117\n",
            "Batch 450: Loss = 0.0848\n",
            "Batch 500: Loss = 0.0598\n",
            "Batch 550: Loss = 0.0575\n",
            "Batch 600: Loss = 0.0629\n",
            "Batch 650: Loss = 0.0443\n",
            "Batch 700: Loss = 0.1337\n",
            "Batch 750: Loss = 0.1150\n",
            "Batch 800: Loss = 0.0792\n",
            "Batch 850: Loss = 0.0950\n",
            "Batch 900: Loss = 0.0982\n",
            "Epoch [1/5], Loss: 0.1147, Student1 Acc: 98.33%, Student2 Acc: 98.04%\n",
            "Batch 0: Loss = 0.1003\n",
            "Batch 50: Loss = 0.0242\n",
            "Batch 100: Loss = 0.0426\n",
            "Batch 150: Loss = 0.0541\n",
            "Batch 200: Loss = 0.0467\n",
            "Batch 250: Loss = 0.0488\n",
            "Batch 300: Loss = 0.0937\n",
            "Batch 350: Loss = 0.0251\n",
            "Batch 400: Loss = 0.0654\n",
            "Batch 450: Loss = 0.0344\n",
            "Batch 500: Loss = 0.0405\n",
            "Batch 550: Loss = 0.1442\n",
            "Batch 600: Loss = 0.0663\n",
            "Batch 650: Loss = 0.0402\n",
            "Batch 700: Loss = 0.0503\n",
            "Batch 750: Loss = 0.0841\n",
            "Batch 800: Loss = 0.0303\n",
            "Batch 850: Loss = 0.0409\n",
            "Batch 900: Loss = 0.0191\n",
            "Epoch [2/5], Loss: 0.0592, Student1 Acc: 98.69%, Student2 Acc: 98.48%\n",
            "Batch 0: Loss = 0.0725\n",
            "Batch 50: Loss = 0.0418\n",
            "Batch 100: Loss = 0.0855\n",
            "Batch 150: Loss = 0.0629\n",
            "Batch 200: Loss = 0.0486\n",
            "Batch 250: Loss = 0.0578\n",
            "Batch 300: Loss = 0.0309\n",
            "Batch 350: Loss = 0.0767\n",
            "Batch 400: Loss = 0.0510\n",
            "Batch 450: Loss = 0.0426\n",
            "Batch 500: Loss = 0.0428\n",
            "Batch 550: Loss = 0.0199\n",
            "Batch 600: Loss = 0.0440\n",
            "Batch 650: Loss = 0.0491\n",
            "Batch 700: Loss = 0.0607\n",
            "Batch 750: Loss = 0.0341\n",
            "Batch 800: Loss = 0.0932\n",
            "Batch 850: Loss = 0.0316\n",
            "Batch 900: Loss = 0.0392\n",
            "Epoch [3/5], Loss: 0.0497, Student1 Acc: 99.03%, Student2 Acc: 98.64%\n",
            "Batch 0: Loss = 0.0932\n",
            "Batch 50: Loss = 0.0179\n",
            "Batch 100: Loss = 0.0540\n",
            "Batch 150: Loss = 0.0400\n",
            "Batch 200: Loss = 0.0392\n",
            "Batch 250: Loss = 0.0328\n",
            "Batch 300: Loss = 0.0196\n",
            "Batch 350: Loss = 0.0606\n",
            "Batch 400: Loss = 0.0334\n",
            "Batch 450: Loss = 0.0231\n",
            "Batch 500: Loss = 0.0390\n",
            "Batch 550: Loss = 0.0297\n",
            "Batch 600: Loss = 0.0669\n",
            "Batch 650: Loss = 0.0117\n",
            "Batch 700: Loss = 0.0744\n",
            "Batch 750: Loss = 0.0709\n",
            "Batch 800: Loss = 0.0646\n",
            "Batch 850: Loss = 0.0448\n",
            "Batch 900: Loss = 0.0460\n",
            "Epoch [4/5], Loss: 0.0437, Student1 Acc: 99.16%, Student2 Acc: 99.18%\n",
            "Batch 0: Loss = 0.0225\n",
            "Batch 50: Loss = 0.0171\n",
            "Batch 100: Loss = 0.0184\n",
            "Batch 150: Loss = 0.0113\n",
            "Batch 200: Loss = 0.0215\n",
            "Batch 250: Loss = 0.0539\n",
            "Batch 300: Loss = 0.0304\n",
            "Batch 350: Loss = 0.0593\n",
            "Batch 400: Loss = 0.0202\n",
            "Batch 450: Loss = 0.0343\n",
            "Batch 500: Loss = 0.0253\n",
            "Batch 550: Loss = 0.0595\n",
            "Batch 600: Loss = 0.0334\n",
            "Batch 650: Loss = 0.0205\n",
            "Batch 700: Loss = 0.1086\n",
            "Batch 750: Loss = 0.0348\n",
            "Batch 800: Loss = 0.0196\n",
            "Batch 850: Loss = 0.0238\n",
            "Batch 900: Loss = 0.0169\n",
            "Epoch [5/5], Loss: 0.0394, Student1 Acc: 99.20%, Student2 Acc: 99.28%\n",
            "\n",
            "Final Test Accuracy - Student1: 99.20%, Student2: 99.28%\n",
            "✓ Both models saved successfully\n"
          ]
        }
      ]
    }
  ]
}